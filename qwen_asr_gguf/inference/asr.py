# coding=utf-8
import os
import time
import re
import codecs
import numpy as np
import multiprocessing as mp
from pathlib import Path
from collections import deque
from typing import Optional, List

from .schema import MsgType, StreamingMessage, DecodeResult, ASREngineConfig, TranscribeResult, ForcedAlignItem, ForcedAlignResult
from .asr_worker import asr_helper_worker_proc
from .utils import normalize_language_name, validate_language
from . import llama

class QwenASREngine:
    """Qwen3-ASR æµå¼è½¬å½•å¼•æ“ (GGUF åç«¯) - ç»Ÿä¸€è¾…åŠ©è¿›ç¨‹æ¶æ„"""
    def __init__(self, config: ASREngineConfig):
        self.verbose = config.verbose
        if self.verbose: print(f"--- [QwenASR] åˆå§‹åŒ–å¼•æ“ (DML: {config.use_dml}) ---")

        from qwen_asr_gguf.inference import llama
        self.llama_mod = llama # keep reference
        
        # è·¯å¾„è§£æ
        llm_gguf = os.path.join(config.model_dir, config.llm_fn)

        # 1. åŠ è½½è¯†åˆ« LLM
        self.model = llama.LlamaModel(llm_gguf)
        self.embedding_table = llama.get_token_embeddings_gguf(llm_gguf)
        self.ctx = llama.LlamaContext(self.model, n_ctx=config.n_ctx, n_batch=4096, embeddings=False)
        
        # 2. å¯åŠ¨ç»Ÿä¸€è¾…åŠ©å­è¿›ç¨‹ (ç¼–ç  + å¯¹é½)
        self.to_worker_q = mp.Queue()
        self.from_enc_q = mp.Queue()
        self.from_align_q = mp.Queue()
        
        self.helper_proc = mp.Process(
            target=asr_helper_worker_proc, 
            args=(self.to_worker_q, self.from_enc_q, self.from_align_q, config), 
            daemon=True
        )
        self.helper_proc.start()
        
        # 3. ç­‰å¾…å­è¿›ç¨‹å°±ç»ªä¿¡å· (åŒ…å« Encoder é¢„çƒ­å®Œæˆ)
        msg = self.from_enc_q.get()
        if msg.msg_type == MsgType.MSG_READY and self.verbose:
            print("--- [QwenASR] è¾…åŠ©è¿›ç¨‹å·²å°±ç»ª ---")

        # ç¼“å­˜ Token ID
        self.ID_IM_START = self.model.token_to_id("<|im_start|>")
        self.ID_IM_END = self.model.token_to_id("<|im_end|>")
        self.ID_AUDIO_START = self.model.token_to_id("<|audio_start|>")
        self.ID_AUDIO_END = self.model.token_to_id("<|audio_end|>")
        self.ID_ASR_TEXT = self.model.token_to_id("<asr_text>")

    def shutdown(self):
        # å‘è¾…åŠ©è¿›ç¨‹å‘é€åœæ­¢ä¿¡å·
        if self.helper_proc:
            self.to_worker_q.put(StreamingMessage(MsgType.CMD_STOP))
            self.helper_proc.join()
        if self.verbose: print("--- [QwenASR] å¼•æ“å·²å…³é—­ ---")

    def _build_prompt_embd(self, audio_embd: np.ndarray, prefix_text: str, context: Optional[str], language: Optional[str]):
        """æ„é€ ç”¨äº LLM è¾“å…¥çš„ Embedding åºåˆ— (åŒºå—åŒ–æ‰“åŒ…æ¨¡å¼)"""
        def tk(t): return self.model.tokenize(t)

        # 1. åŒºå— A: éŸ³é¢‘ä¹‹å‰çš„æ‰€æœ‰å†…å®¹ (System + User Header)
        prefix_str = f"system\n{context or 'You are a helpful assistant.'}"
        prefix_tokens = [self.ID_IM_START] + tk(prefix_str) + [self.ID_IM_END] + \
                        [self.ID_IM_START] + tk("user\n") + [self.ID_AUDIO_START]
        
        # 2. åŒºå— B: éŸ³é¢‘ä¹‹åçš„æ‰€æœ‰å†…å®¹ (Instruction + Assistant Header + History)
        suffix_head = f"assistant\n"
        if language: suffix_head += f"language {language}"
        
        suffix_tokens = [self.ID_AUDIO_END] + [self.ID_IM_END] + \
                        [self.ID_IM_START] + tk(suffix_head) + [self.ID_ASR_TEXT] + tk(prefix_text)

        # 3. ç»Ÿè®¡å¹¶æ‹¼æ¥
        n_pre, n_aud, n_suf = len(prefix_tokens), audio_embd.shape[0], len(suffix_tokens)
        total_embd = np.zeros((n_pre + n_aud + n_suf, self.model.n_embd), dtype=np.float32)
        
        total_embd[:n_pre] = self.embedding_table[prefix_tokens]
        total_embd[n_pre : n_pre + n_aud] = audio_embd
        total_embd[n_pre + n_aud:] = self.embedding_table[suffix_tokens]
        
        return total_embd

    def _run_llm_buffered(
        self, 
        full_embd: np.ndarray,
        prefix_text: str, 
        rollback_num: int,
        is_last_chunk: bool = False, 
        temperature: float = 0.4
    ) -> DecodeResult:
        """å†…éƒ¨æ–¹æ³•ï¼šæ‰§è¡Œå•æ¬¡ LLM ç”Ÿæˆå¾ªç¯ï¼ˆä»…è´Ÿè´£æ¨ç†ï¼‰"""
        result = DecodeResult()
        
        total_len = full_embd.shape[0]
        pos_base = np.arange(0, total_len, dtype=np.int32)
        pos_arr = np.concatenate([pos_base, pos_base, pos_base, np.zeros(total_len, dtype=np.int32)])
        batch = self.llama_mod.LlamaBatch(max(total_len * 4, 8192), self.model.n_embd, 1)
        batch.set_embd(full_embd, pos=pos_arr)
        
        # 1. Prefill
        self.ctx.clear_kv_cache()
        t_pre_start = time.time()
        self.ctx.decode(batch)
        prefill_time = time.time() - t_pre_start
        
        # 2. Generation Loopï¼ˆä½¿ç”¨æ–°é‡‡æ ·å™¨å’Œéšæœºç§å­ï¼‰
        t_gen_start = time.time()
        n_gen_tokens = 0
        display_queue = deque()
        stable_tokens = []
        stable_text_acc = ""
        cur_pos = total_len
        gen_batch = self.llama_mod.LlamaBatch(4, 0, 1)
        text_decoder = codecs.getincrementaldecoder('utf-8')(errors='replace')
        
        # æ¯æ¬¡è§£ç ä½¿ç”¨æ–°çš„éšæœºç§å­
        seed = int(np.random.randint(0, 2**31 - 1))
        sampler = self.llama_mod.LlamaSampler(temperature=temperature, seed=seed)
        last_sampled_token = sampler.sample(self.ctx.ptr)
        for _ in range(150): # Max new tokens per chunk
            if last_sampled_token in [self.model.eos_token, self.ID_IM_END]:
                break
            
            gen_batch.set_token(last_sampled_token, pos=np.array([cur_pos, cur_pos, cur_pos, 0], dtype=np.int32))
            self.ctx.decode(gen_batch)
            
            display_queue.append(last_sampled_token)
            if len(display_queue) > rollback_num:
                ready_token = display_queue.popleft()
                stable_tokens.append(ready_token)
                piece = text_decoder.decode(self.model.token_to_bytes(ready_token))
                if piece:
                    print(re.sub('([ï¼Œã€‚ï¼Ÿï¼])', '\\1\n', piece), end='', flush=True)
                    stable_text_acc += piece
            
            # ç†”æ–­æ£€æŸ¥ï¼šæ£€æµ‹é‡å¤å¾ªç¯
            if len(stable_tokens) > 15:
                if len(set(stable_tokens[-15:])) <= 3:
                    result.is_aborted = True
                    break
            
            cur_pos += 1
            last_sampled_token = sampler.sample(self.ctx.ptr)
            n_gen_tokens += 1
            
        gen_time = time.time() - t_gen_start
        del sampler  # é‡Šæ”¾é‡‡æ ·å™¨èµ„æº
            
        if is_last_chunk and not result.is_aborted:
            while display_queue:
                t = display_queue.popleft()
                stable_tokens.append(t)
                piece = text_decoder.decode(self.model.token_to_bytes(t))
                if piece:
                    print(re.sub('([ï¼Œã€‚ï¼Ÿï¼])', '\\1\n', piece), end="", flush=True)
                    stable_text_acc += piece
            final_p = text_decoder.decode(b"", final=True)
            if final_p: 
                print(final_p, end='', flush=True)
                stable_text_acc += final_p
        
        # å¡«å……ç»“æœï¼ˆå†…æ ¸è¾“å‡ºæ ‡å‡†åŒ–ï¼‰
        result.text = prefix_text + stable_text_acc
        result.stable_tokens = stable_tokens
        result.t_prefill = prefill_time / 1000
        result.t_generate = gen_time
        result.n_prefill = total_len
        result.n_generate = n_gen_tokens
        return result

    def transcribe(
        self, 
        audio: np.ndarray,
        context: str = "",
        language: str = "Chinese",
        chunk_size_sec: float = 40.0,
        memory_chunks: int = 2,
        temperature: float = 0.4,
        rollback_num: int = 5
    ) -> TranscribeResult:
        """è¿è¡Œå®Œæ•´è½¬å½•æµæ°´çº¿ (å¼‚æ­¥å¯¹é½ - å•é€šé“ç‰ˆ)"""
        # è¯­è¨€å½’ä¸€åŒ–ä¸æ ¡éªŒ
        if language:
            language = normalize_language_name(language)
            validate_language(language)

        sr = 16000
        samples_per_chunk = int(chunk_size_sec * sr)
        total_len = len(audio)
        num_chunks = int(np.ceil(total_len / samples_per_chunk))
        
        history_segments = deque(maxlen=memory_chunks)
        total_full_text = ""
        all_aligned_items: List[ForcedAlignItem] = []
        align_tasks_count = 0
        
        # ç»Ÿè®¡æŒ‡æ ‡
        stats = {
            "prefill_time": 0.0, "decode_time": 0.0,
            "prefill_tokens": 0, "decode_tokens": 0,
            "wait_time": 0.0, "encode_time": 0.0,
            "align_enc_time": 0.0, "align_dec_time": 0.0
        }
        t_main_start = time.time()

        def send_enc_chunk(idx):
            s, e = idx * samples_per_chunk, min((idx + 1) * samples_per_chunk, total_len)
            data = audio[s:e]
            if len(data) < samples_per_chunk: 
                data = np.pad(data, (0, samples_per_chunk - len(data)))
            self.to_worker_q.put(StreamingMessage(MsgType.CMD_ENCODE, data=data, is_last=(idx == num_chunks - 1)))

        def send_align_task(idx, text, is_last):
            nonlocal align_tasks_count
            if (self.helper_proc and self.helper_proc.is_alive()) and text.strip():
                s, e = idx * samples_per_chunk, min((idx + 1) * samples_per_chunk, total_len)
                audio_slice = audio[s:e]
                
                self.to_worker_q.put(StreamingMessage(
                    msg_type=MsgType.CMD_ALIGN,
                    data=audio_slice,
                    text=text,
                    offset_sec=float(idx * chunk_size_sec),
                    language=language,
                    is_last=is_last
                ))
                align_tasks_count += 1

        if num_chunks > 0: send_enc_chunk(0)

        for i in range(num_chunks):
            # 1. è·å–ç‰¹å¾
            t_w_start = time.time()
            msg = self.from_enc_q.get()
            stats["wait_time"] += (time.time() - t_w_start)
            stats["encode_time"] += msg.encode_time
            
            current_embd = msg.data
            was_last = msg.is_last
            
            # æå‰è§¦å‘ä¸‹ä¸€å—ç‰¹å¾æå–
            if not was_last: send_enc_chunk(i + 1)
            
            # 2. æ„å»ºè®°å¿†å¹¶æ¨ç†
            prefix_text = "".join([seg['text'] for seg in history_segments])
            combined_audio_embd = np.concatenate([seg['embd'] for seg in history_segments] + [current_embd], axis=0)
            full_embd = self._build_prompt_embd(combined_audio_embd, prefix_text, context, language)
            
            temp = temperature
            for retry in range(6):
                res = self._run_llm_buffered(full_embd, prefix_text, rollback_num, is_last_chunk=was_last, temperature=temp)
                if not res.is_aborted: break
                temp += 0.3
                if self.verbose: print(f"\n[ASR] ç†”æ–­é‡å¯ (Temp={temp:.1f})")
            
            new_text_part = res.text[len(prefix_text):]
            history_segments.append({'embd': current_embd, 'text': new_text_part})
            total_full_text += new_text_part
            
            # --- å¼‚æ­¥ä¸‹å‘å¯¹é½ä»»åŠ¡ ---
            send_align_task(i, new_text_part, was_last)

            stats["prefill_tokens"] += res.n_prefill; stats["prefill_time"] += res.t_prefill
            stats["decode_tokens"] += res.n_generate; stats["decode_time"] += res.t_generate

        # 3. å›æ”¶æ‰€æœ‰å¯¹é½ç»“æœ
        if align_tasks_count > 0:
            if self.verbose: print(f"\n--- [QwenASR] æ­£åœ¨å›æ”¶ {align_tasks_count} ä¸ªå¯¹é½ç»“æœ... ---")
            for _ in range(align_tasks_count):
                align_msg = self.from_align_q.get()
                if align_msg.msg_type == MsgType.MSG_ALIGN and align_msg.data:
                    align_res: ForcedAlignResult = align_msg.data
                    all_aligned_items.extend(align_res.items)
                    if align_res.performance:
                        stats["align_enc_time"] += align_res.performance.get("encoder_time", 0)
                        stats["align_dec_time"] += align_res.performance.get("decoder_time", 0)

        # 4. æ’åºç»“æœ (é˜²æ­¢å­è¿›ç¨‹å›æ”¶ä¹±åº)
        all_aligned_items.sort(key=lambda x: x.start_time)

        t_total = time.time() - t_main_start
        audio_duration = total_len / sr

        if self.verbose:
            rtf = t_total / audio_duration if audio_duration > 0 else 0
            pre_speed = stats["prefill_tokens"] / (stats["prefill_time"]) if stats["prefill_time"] > 0 else 0
            gen_speed = stats["decode_tokens"] / (stats["decode_time"]) if stats["decode_time"] > 0 else 0
            
            print(f"\n\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            print(f"  ğŸ”¹ RTF (å®æ—¶ç‡) : {rtf:.3f} (è¶Šå°è¶Šå¿«)")
            print(f"  ğŸ”¹ éŸ³é¢‘æ—¶é•¿    : {audio_duration:.2f} ç§’")
            print(f"  ğŸ”¹ æ€»å¤„ç†è€—æ—¶  : {t_total:.2f} ç§’")
            print(f"  ğŸ”¹ ç¼–ç ç­‰å¾…    : {stats['wait_time']:.2f} ç§’")
            if self.helper_proc:
                print(f"  ğŸ”¹ å¯¹é½æ€»æ—¶    : {stats['align_enc_time']+stats['align_dec_time']:.2f} ç§’ (å­è¿›ç¨‹å¹¶è¡Œ Enc:{stats['align_enc_time']:.2f}s, Dec:{stats['align_dec_time']:.2f}s)")
            print(f"  ğŸ”¹ LLM é¢„å¡«å……  : {stats['prefill_time']:.3f} ç§’ ({stats['prefill_tokens']} tokens, {pre_speed:.1f} tokens/s)")
            print(f"  ğŸ”¹ LLM ç”Ÿæˆ    : {stats['decode_time']:.3f} ç§’ ({stats['decode_tokens']} tokens, {gen_speed:.1f} tokens/s)")
            
        return TranscribeResult(
            text=total_full_text,
            alignment=ForcedAlignResult(items=all_aligned_items) if all_aligned_items else None,
            performance=stats
        )
