# å®æˆ˜æ¡ˆä¾‹ï¼šåœ¨ macOS ä¸Šè·‘é€š GGUF æ¨ç†è·¯å¾„

> æœ¬æ–‡æ˜¯ [PyTorch è·¯å¾„å®æˆ˜](./PRACTICAL_CASE_PYTORCH_INFERENCE.md) çš„ç»­ç¯‡ã€‚
> æˆ‘ä»¬å·²ç»ç”¨ PyTorch è·¯å¾„æˆåŠŸè½¬å½•äº†éŸ³é¢‘ï¼Œæœ¬æ–‡è®°å½•å¦‚ä½•èµ°é€š
> **è¿™ä¸ªé¡¹ç›®çœŸæ­£çš„æ ¸å¿ƒè·¯å¾„**â€”â€”GGUF æ¨ç†â€”â€”ä»¥åŠé€”ä¸­è¸©è¿‡çš„æ¯ä¸€ä¸ªå‘ã€‚

---

## èƒŒæ™¯ä¸ç›®æ ‡

ä¸Šä¸€ç¯‡æ–‡æ¡£å·²ç»ï¼š

- æŠŠ `test_audio.wav`ï¼ˆ16 ç§’ï¼Œ16kHz å•å£°é“ï¼‰å‡†å¤‡å°±ç»ª
- å®‰è£…äº† torchã€onnxruntimeã€transformers==4.57.6 ç­‰ä¾èµ–
- ç”¨ PyTorch è·¯å¾„å®Œæˆäº†è½¬å½•ï¼ŒRTF â‰ˆ 0.43

**æœ¬ç¯‡ç›®æ ‡**ï¼šæ”¹ç”¨ `transcribe.py`ï¼ˆGGUF è·¯å¾„ï¼‰è½¬å½•åŒä¸€æ®µéŸ³é¢‘ï¼Œ
ä½“éªŒ GGUF æ–¹æ¡ˆåœ¨é€Ÿåº¦ä¸Šçš„å®é™…å·®è·ã€‚

GGUF è·¯å¾„éœ€è¦ä¸¤æ ·ä¸œè¥¿ï¼š

```
éœ€æ±‚ 1ï¼šmodel/ ç›®å½•           éœ€æ±‚ 2ï¼šqwen_asr_gguf/inference/bin/
â”œâ”€â”€ qwen3_asr_llm.q4_k.gguf   â”œâ”€â”€ libllama.dylib
â”œâ”€â”€ qwen3_asr_encoder_frontend.int4.onnx
â””â”€â”€ qwen3_asr_encoder_backend.int4.onnx
```

---

## ç¬¬ä¸€æ­¥ï¼šæ’æŸ¥å·²æœ‰æ¡ä»¶

```bash
ls model/          # â†’ ä¸å­˜åœ¨
ls qwen_asr_gguf/inference/bin/   # â†’ ä¸å­˜åœ¨
```

ä¸¤æ ·ä¸œè¥¿éƒ½ç¼ºï¼Œéœ€è¦ä»å¤´è·å–ã€‚

---

## ç¬¬äºŒæ­¥ï¼šä¸‹è½½ GGUF æ¨¡å‹æ–‡ä»¶

é¡¹ç›®åœ¨ GitHub Releases å‘å¸ƒäº†ä¸¤ä¸ª tagï¼š

```bash
gh release list --repo HaujetZhao/Qwen3-ASR-GGUF

# è¾“å‡ºï¼š
# Qwen3-ASR-Transcribe è½¬å½•å·¥å…·  Latest  v0.1   2026-02-22  (Windows å¯æ‰§è¡Œæ–‡ä»¶)
# GGUF æ¨¡å‹ä¸‹è½½                          models  2026-02-21  (æ¨¡å‹æ–‡ä»¶)
```

`models` tag ä¸‹æœ‰ï¼š
- `Qwen3-ASR-0.6B-gguf.zip`ï¼ˆ538MBï¼‰
- `Qwen3-ASR-1.7B-gguf.zip`
- `Qwen3-ForceAligner-0.6B-gguf.zip`

ä¸‹è½½ 0.6B æ¨¡å‹åŒ…ï¼š

```bash
gh release download models --repo HaujetZhao/Qwen3-ASR-GGUF \
    --pattern "Qwen3-ASR-0.6B-gguf.zip" \
    --dir /tmp/gguf_dl \
    --clobber
```

> **è¸©å‘**ï¼šç¬¬ä¸€æ¬¡ä¸‹è½½å¾—åˆ°äº† 111MB çš„æ–‡ä»¶ï¼Œç›´æ¥è§£å‹æŠ¥é”™
> `End-of-central-directory signature not found`ï¼ˆæ–‡ä»¶æŸåï¼‰ã€‚
> åŸå› ï¼š`gh release download` å‘½ä»¤åœ¨åå°è¿è¡Œæ—¶è¢«æå‰ä¸­æ–­ï¼Œæ–‡ä»¶æœªä¸‹è½½å®Œæ•´ã€‚
> **è§£å†³**ï¼šå‰å°åŒæ­¥æ‰§è¡Œï¼Œç­‰å¾…å®Œæ•´ä¸‹è½½ï¼ˆçœŸå®å¤§å° 538MBï¼‰å†è§£å‹ã€‚

è§£å‹å¹¶æ”¾å…¥ `model/` ç›®å½•ï¼š

```bash
mkdir -p model
unzip /tmp/gguf_dl/Qwen3-ASR-0.6B-gguf.zip -d /tmp/gguf_extract/
cp /tmp/gguf_extract/*.onnx /tmp/gguf_extract/*.gguf ./model/
```

è§£å‹åçš„æ–‡ä»¶ï¼š

```
model/
â”œâ”€â”€ qwen3_asr_encoder_backend.int4.onnx   90 MB   (Encoder åç«¯ï¼ŒTransformer å±‚)
â”œâ”€â”€ qwen3_asr_encoder_frontend.int4.onnx  19 MB   (Encoder å‰ç«¯ï¼ŒCNN å±‚)
â””â”€â”€ qwen3_asr_llm.q4_k.gguf             462 MB   (Decoderï¼Œq4_k é‡åŒ–)
```

> **ä¸ PyTorch æ¨¡å‹çš„å¤§å°å¯¹æ¯”**ï¼š
> PyTorch åŸå§‹æƒé‡ 1.8 GB (fp32) â†’ GGUF å¥—ä»¶åˆè®¡ 571 MBï¼ˆint4é‡åŒ–ï¼‰
> ä½“ç§¯å‹ç¼©åˆ°çº¦ **32%**ï¼ŒåŒæ—¶é‡åŒ–æŸå¤±æå°ï¼ˆå›°æƒ‘åº¦ä»…å¢åŠ  8.7%ï¼‰ã€‚

---

## ç¬¬ä¸‰æ­¥ï¼šè·å–é¢„ç¼–è¯‘çš„ libllama.dylib

GGUF è·¯å¾„çš„ Decoder é€šè¿‡ `llama.py` ç”¨ ctypes ç›´æ¥è°ƒç”¨ `libllama.dylib`ã€‚
è¿™ä¸ªåŠ¨æ€åº“éœ€è¦æ‰‹åŠ¨æä¾›ï¼Œé¡¹ç›®æ²¡æœ‰æ‰“åŒ…è¿›æºç ï¼ˆä»…åœ¨ Windows Release åŒ…é‡Œæœ‰ï¼‰ã€‚

### æ–¹æ¡ˆ Aï¼šä» llama-cpp-python å€Ÿç”¨ï¼ˆå¤±è´¥ï¼‰

æœ€ç®€å•çš„æ–¹æ¡ˆæ˜¯å®‰è£… `llama-cpp-python`ï¼Œå®ƒåœ¨å®‰è£…æ—¶ä¼šç¼–è¯‘å¹¶æ†ç»‘ `libllama.dylib`ï¼š

```bash
CMAKE_ARGS="-DGGML_METAL=on" pip3 install llama-cpp-python --break-system-packages
```

å®‰è£…å®Œæˆåï¼Œåœ¨ä»¥ä¸‹è·¯å¾„æ‰¾åˆ°æ‰€æœ‰ dylibï¼š

```
/opt/homebrew/lib/python3.11/site-packages/llama_cpp/lib/
â”œâ”€â”€ libllama.dylib
â”œâ”€â”€ libggml.dylib
â”œâ”€â”€ libggml-base.dylib
â”œâ”€â”€ libggml-blas.dylib
â”œâ”€â”€ libggml-cpu.dylib
â””â”€â”€ libggml-metal.dylib
```

å¤åˆ¶åˆ°é¡¹ç›®çš„ `bin/` ç›®å½•ï¼š

```bash
mkdir -p qwen_asr_gguf/inference/bin
cp /opt/homebrew/lib/python3.11/site-packages/llama_cpp/lib/lib*.dylib \
   qwen_asr_gguf/inference/bin/
```

è¿è¡Œæ—¶ç¬¬ä¸€æ¬¡æŠ¥é”™ï¼ˆç¼º libggml-blas.dylibï¼‰ï¼š

```
dlopen(libggml.dylib): Library not loaded: @rpath/libggml-blas.dylib
```

è¡¥å……å¤åˆ¶ `libggml-blas.dylib` åï¼Œç¬¬äºŒæ¬¡å‡ºç° segfaultï¼ˆexit code 139ï¼‰ï¼š

```
--- [QwenASR] åˆå§‹åŒ–å¼•æ“ (DML: False) ---
[è¿›ç¨‹å´©æºƒï¼Œæ— è¾“å‡º]
```

**åŸå› åˆ†æ**ï¼š`llama-cpp-python 0.3.16` å¯¹åº”çš„ llama.cpp ç‰ˆæœ¬è¾ƒæ—§ï¼Œ
å…¶ C ç»“æ„ä½“å¸ƒå±€ä¸é¡¹ç›® `llama.py` ä¸­å®šä¹‰çš„ä¸ä¸€è‡´ï¼Œå¯¼è‡´å†…å­˜è¶Šç•Œå´©æºƒã€‚

å…·ä½“å·®å¼‚ï¼šé¡¹ç›® `llama.py` çš„ `llama_context_params` ç»“æ„ä½“åŒ…å«è¿™äº›æ–°å­—æ®µï¼š

```python
("flash_attn_type", ctypes.c_int32),   # æšä¸¾ç±»å‹ï¼ˆæ—§ç‰ˆæ˜¯ boolï¼‰
("op_offload", ctypes.c_bool),
("swa_full", ctypes.c_bool),           # PR #13194 æ–°å¢
("kv_unified", ctypes.c_bool),         # PR #14363 æ–°å¢
("samplers", ctypes.POINTER(...)),      # æ–°å¢ sampler é…ç½®
("n_samplers", ctypes.c_size_t),
```

è¿™äº›å­—æ®µåœ¨ `llama-cpp-python 0.3.16` æ‰€ç”¨çš„ llama.cpp ç‰ˆæœ¬ä¸­å°šä¸å­˜åœ¨ã€‚
**ç»“è®º**ï¼šä¸èƒ½å€Ÿç”¨æ—§ç‰ˆæœ¬çš„ dylibï¼Œå¿…é¡»ä»åŒ¹é…ç‰ˆæœ¬çš„æºç ç¼–è¯‘ã€‚

---

### æ–¹æ¡ˆ Bï¼šä»é¡¹ç›®è‡ªå¸¦çš„ ref/llama.cpp ç¼–è¯‘ï¼ˆéƒ¨åˆ†å¤±è´¥ï¼‰

é¡¹ç›®åœ¨ `ref/llama.cpp/` ä¸‹ä¿å­˜äº†å¯¹åº”çš„ llama.cpp æºç å¿«ç…§ï¼Œ
å…¶ `include/llama.h` åŒ…å«äº†ä¸Šè¿°æ‰€æœ‰æ–°å­—æ®µï¼Œç‰ˆæœ¬åŒ¹é…ã€‚

å°è¯• cmake é…ç½®ï¼š

```bash
cmake -S ref/llama.cpp -B /tmp/llama_build \
    -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON
```

æŠ¥é”™ï¼š

```
The source directory does not contain a CMakeLists.txt file.
```

**åŸå› **ï¼š`ref/llama.cpp/` æ˜¯ä¸€ä»½ä¸å®Œæ•´çš„æºç å¿«ç…§â€”â€”
æ ¹ç›®å½•çš„ `CMakeLists.txt` ç¼ºå¤±ï¼Œ`tools/` å­ç›®å½•ä¹Ÿä¸å­˜åœ¨ã€‚
è¿™ä»½ä»£ç å¯èƒ½æ˜¯é€šè¿‡é€‰æ‹©æ€§å¤åˆ¶éƒ¨åˆ†æ–‡ä»¶å¾—åˆ°çš„ï¼Œå¹¶éå®Œæ•´ cloneã€‚

å°è¯•ä» GitHub å…‹éš†æœ€æ–° master çš„ `CMakeLists.txt` è¡¥å…¥ï¼Œä»æŠ¥é”™ï¼š

```
The source directory does not contain a CMakeLists.txt file.
# ï¼ˆtools/ ç›®å½•ä¾ç„¶ç¼ºå¤±ï¼Œcmake é˜¶æ®µå¤±è´¥ï¼‰
```

**ç»“è®º**ï¼š`ref/llama.cpp` æ— æ³•ç›´æ¥ç”¨äºç¼–è¯‘ï¼Œéœ€è¦å®Œæ•´ä»“åº“ã€‚

---

### æ–¹æ¡ˆ Cï¼šå…‹éš†å®Œæ•´ llama.cpp ç¼–è¯‘ï¼ˆæˆåŠŸï¼‰

```bash
git clone --depth 1 https://github.com/ggml-org/llama.cpp.git /tmp/llama_full
```

cmake é…ç½®ï¼ˆå…³é—­ BLAS é¿å…é¢å¤–ä¾èµ–ï¼Œå¼€å¯ Metal åˆ©ç”¨ Apple Siliconï¼‰ï¼š

```bash
cmake -S /tmp/llama_full -B /tmp/lb3 \
    -DCMAKE_BUILD_TYPE=Release \
    -DBUILD_SHARED_LIBS=ON \
    -DLLAMA_BUILD_TESTS=OFF \
    -DLLAMA_BUILD_EXAMPLES=OFF \
    -DGGML_METAL=ON \
    -DGGML_BLAS=OFF
```

ç¼–è¯‘ï¼ˆä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒï¼Œçº¦ 2-3 åˆ†é’Ÿï¼‰ï¼š

```bash
cmake --build /tmp/lb3 --config Release \
    -j$(sysctl -n hw.logicalcpu) \
    --target llama ggml
```

ç¼–è¯‘äº§å‡ºï¼š

```
/tmp/lb3/bin/
â”œâ”€â”€ libllama.dylib      2.0 MB
â”œâ”€â”€ libggml.dylib        58 KB
â”œâ”€â”€ libggml-base.dylib  637 KB
â”œâ”€â”€ libggml-cpu.dylib   875 KB
â””â”€â”€ libggml-metal.dylib 764 KB
```

å¤åˆ¶åˆ°é¡¹ç›® `bin/` ç›®å½•ï¼ˆè¦†ç›–æ—§æ–‡ä»¶ï¼‰ï¼š

```bash
cp /tmp/lb3/bin/libllama.dylib \
   /tmp/lb3/bin/libggml.dylib \
   /tmp/lb3/bin/libggml-base.dylib \
   /tmp/lb3/bin/libggml-cpu.dylib \
   /tmp/lb3/bin/libggml-metal.dylib \
   qwen_asr_gguf/inference/bin/
```

---

## ç¬¬å››æ­¥ï¼šè¿è¡Œ GGUF æ¨ç†

```bash
python3 transcribe.py test_audio.wav \
    --model-dir ./model \
    --prec int4 \
    --no-dml \      # macOS ä¸æ”¯æŒ DirectMLï¼ˆWindows ä¸“ç”¨ï¼‰
    --no-vulkan \   # æœ¬æ¬¡ä¸éœ€è¦ Vulkan åŠ é€Ÿ
    --no-ts \       # ä¸å¯ç”¨æ—¶é—´æˆ³å¯¹é½ï¼ˆèŠ‚çœæ—¶é—´ï¼Œå…ˆè·‘é€šåŸºæœ¬è·¯å¾„ï¼‰
    -y              # è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶
```

è¾“å‡ºï¼š

```
â•­â”€â”€â”€â”€â”€â”€â”€â”€ Qwen3-ASR é…ç½®é€‰é¡¹ â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  æ¨¡å‹ç›®å½•    ./model               â”‚
â”‚  ç¼–ç ç²¾åº¦    int4                  â”‚
â”‚  åŠ é€Ÿè®¾å¤‡    DML:OFF | Vulkan:OFF  â”‚
â”‚  æ—¶é—´æˆ³å¯¹é½  ç¦ç”¨                  â”‚
â”‚  è¯­è¨€è®¾å®š    è‡ªåŠ¨è¯†åˆ«              â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
--- [QwenASR] åˆå§‹åŒ–å¼•æ“ (DML: False) ---
--- [QwenASR] è¾…åŠ©è¿›ç¨‹å·²å°±ç»ª ---
--- [QwenASR] å¼•æ“åˆå§‹åŒ–è€—æ—¶: 8.03 ç§’ ---

å¼€å§‹å¤„ç†: test_audio.wav

Okay,
 é‚£åº”è¯¥æ˜¯æœ‰å·²ç»æœ‰äº†è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œ
å‘ƒï¼Œ
ä¸ºæ‰€æœ‰ç«¯å†™ä¸€ä»½æ¶æ„æ–‡æ¡£ï¼Œ
è®©æˆ‘å¿«é€Ÿåœ°ç†è§£è¿™ä¸ªé¡¹ç›®ã€‚

ğŸ“Š æ€§èƒ½ç»Ÿè®¡:
  ğŸ”¹ RTF (å®æ—¶ç‡) : 0.104 (è¶Šå°è¶Šå¿«)
  ğŸ”¹ éŸ³é¢‘æ—¶é•¿    : 15.96 ç§’
  ğŸ”¹ æ€»å¤„ç†è€—æ—¶  : 1.66 ç§’
  ğŸ”¹ ç¼–ç ç­‰å¾…    : 0.86 ç§’
  ğŸ”¹ LLM é¢„å¡«å……  : 0.542 ç§’ (540 tokens, 995.8 tokens/s)
  ğŸ”¹ LLM ç”Ÿæˆ    : 0.251 ç§’ (29 tokens, 115.7 tokens/s)
âœ… å·²ä¿å­˜æ–‡æœ¬æ–‡ä»¶: test_audio.txt
--- [QwenASR] å¼•æ“å·²å…³é—­ ---
```

---

## ç¬¬äº”æ­¥ï¼šå¯¹æ¯”ä¸¤æ¡è·¯å¾„çš„æ€§èƒ½

| æŒ‡æ ‡ | PyTorch è·¯å¾„ | GGUF è·¯å¾„ | å·®è· |
|------|:-----------:|:---------:|:----:|
| æ¨¡å‹å¤§å° | 1.8 GB (fp32) | 571 MB (int4) | GGUF å° 3.1x |
| æ¨¡å‹åŠ è½½è€—æ—¶ | 3.8 ç§’ | 8.0 ç§’ï¼ˆå«å­è¿›ç¨‹é¢„çƒ­ï¼‰| PyTorch ç•¥å¿« |
| è½¬å½•è€—æ—¶ï¼ˆ16séŸ³é¢‘ï¼‰| 6.8 ç§’ | **1.66 ç§’** | **GGUF å¿« 4.1x** |
| RTFï¼ˆå®æ—¶ç‡ï¼‰| 0.43 | **0.104** | **GGUF å¿« 4.1x** |
| ä¾èµ– | PyTorch + transformers | onnxruntime + libllama | GGUF æ›´è½» |
| è·¨å¹³å°åŠ é€Ÿ | MPS/CUDA | Metal/DirectML | ä¸åŒ |

**å¤‡æ³¨**ï¼šæ¨¡å‹åŠ è½½ GGUF ç¨æ…¢ï¼Œå› ä¸ºå®ƒéœ€è¦é¢å¤–å¯åŠ¨ä¸€ä¸ªå­è¿›ç¨‹ï¼ˆONNX Encoder Workerï¼‰å¹¶å®Œæˆé¢„çƒ­ï¼ˆè·‘ä¸€æ¬¡ç©ºæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¡æ€§æˆæœ¬ã€‚å¯¹äºé•¿éŸ³é¢‘æˆ–æ‰¹é‡å¤„ç†åœºæ™¯ï¼Œè¿™ä¸ªå·®è·å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚

---

## æ·±å…¥ç†è§£ï¼šGGUF å¼•æ“å¯åŠ¨æ—¶å‘ç”Ÿäº†ä»€ä¹ˆ

`QwenASREngine.__init__()` åšäº†ä¸‰ä»¶äº‹ï¼š

```python
# 1. å¯åŠ¨è¾…åŠ©å­è¿›ç¨‹ï¼ˆè¿è¡Œ ONNX Encoderï¼‰
self.helper_proc = mp.Process(
    target=asr_helper_worker_proc,    # â†’ encoder.py
    args=(to_worker_q, from_enc_q, ..., config),
    daemon=True
)
self.helper_proc.start()

# 2. åœ¨ä¸»è¿›ç¨‹åŠ è½½ GGUF Decoderï¼ˆé€šè¿‡ ctypes è°ƒç”¨ libllama.dylibï¼‰
self.model = llama.LlamaModel(llm_gguf)          # åŠ è½½ .gguf æ–‡ä»¶
self.embedding_table = llama.get_token_embeddings_gguf(llm_gguf)
self.ctx = llama.LlamaContext(self.model, n_ctx=2048, ...)

# 3. ç­‰å¾…å­è¿›ç¨‹å°±ç»ªä¿¡å·ï¼ˆå­è¿›ç¨‹å®Œæˆ ONNX æ¨¡å‹åŠ è½½ + é¢„çƒ­åå‘å‡ºï¼‰
msg = self.from_enc_q.get()  # é˜»å¡ç­‰å¾…
```

**ä¸ºä»€ä¹ˆ Encoder è¦æ”¾åœ¨å­è¿›ç¨‹ï¼Ÿ**

- Encoderï¼ˆONNXï¼‰å’Œ Decoderï¼ˆllama.cppï¼‰æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„æ¨ç†æ¡†æ¶
- å­è¿›ç¨‹éš”ç¦»é¿å…äº†ä¸¤ä¸ªæ¡†æ¶ä¹‹é—´çš„çº¿ç¨‹å†²çª
- æµå¼å¤„ç†æ—¶ï¼ŒEncoder å¯ä»¥æå‰ç¼–ç ä¸‹ä¸€æ®µéŸ³é¢‘ï¼Œ
  å’Œ Decoder ç”Ÿæˆæ–‡æœ¬å½¢æˆ**æµæ°´çº¿å¹¶è¡Œ**ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
- é€šè¿‡ `multiprocessing.Queue` é€šä¿¡ï¼Œ`MSG_EMBD` æ¶ˆæ¯æºå¸¦ audio_embedding

**æ¨ç†é˜¶æ®µçš„æ•°æ®æµï¼ˆä»¥ 16 ç§’éŸ³é¢‘ä¸ºä¾‹ï¼‰**ï¼š

```
ä¸»è¿›ç¨‹                              å­è¿›ç¨‹ï¼ˆEncoder Workerï¼‰
  â”‚                                   â”‚
  â”‚â”€ CMD_ENCODEï¼ˆå‘é€éŸ³é¢‘æ•°æ®ï¼‰â”€â”€â”€â”€â”€â”€â†’â”‚
  â”‚                                   â”œâ”€ ONNX Frontendï¼ˆCNNï¼‰ 0.3s
  â”‚                                   â”œâ”€ ONNX Backendï¼ˆTransformerï¼‰ 0.5s
  â”‚â†â”€ MSG_EMBDï¼ˆè¿”å› audio_embeddingï¼‰â”‚  ç¼–ç ç­‰å¾…: 0.86s
  â”‚
  â”œâ”€ _build_prompt_embd()
  â”‚   æ„å»º: [BOS][system][user][audio_embd][language...][text]
  â”‚
  â”œâ”€ llama_decode()ï¼ˆprefillï¼‰   0.54s  540 tokens
  â”‚
  â””â”€ llama_decode() Ã— 29æ¬¡ï¼ˆgenerateï¼‰ 0.25s  â†’ 29 tokens
      æ¯æ¬¡å– logits â†’ é‡‡æ · â†’ å–ä¸‹ä¸€ä¸ª token
      é‡åˆ° </s> æˆ– <|im_end|> åœæ­¢
```

---

## é‡åˆ°çš„å‘åŠè§£å†³æ–¹æ¡ˆæ±‡æ€»

| é—®é¢˜ | ç°è±¡ | åŸå›  | è§£å†³ |
|------|------|------|------|
| ä¸‹è½½ä¸­æ–­ | unzip æŠ¥é”™ã€Œé zip æ–‡ä»¶ã€ | åå°ä¸‹è½½æœªå®Œæˆ | å‰å°åŒæ­¥ä¸‹è½½ |
| dylib ç¼ºå¤± | `Library not loaded: @rpath/libggml-blas.dylib` | å¤åˆ¶ dylib ä¸å®Œæ•´ | è¡¥å……å¤åˆ¶æ‰€æœ‰ä¾èµ– |
| API ç‰ˆæœ¬ä¸å…¼å®¹ | segfaultï¼ˆexit code 139ï¼‰ | llama-cpp-python ç‰ˆæœ¬è¿‡æ—§ï¼Œstruct å¸ƒå±€ä¸åŒ | ä»æºç ç¼–è¯‘åŒ¹é…ç‰ˆæœ¬ |
| ref/llama.cpp ä¸å®Œæ•´ | cmake æŠ¥é”™ã€Œæ—  CMakeLists.txtã€ | æºç ç›®å½•åªæ˜¯éƒ¨åˆ†å¿«ç…§ | å…‹éš†å®Œæ•´ä»“åº“ |
| cmake ç¼“å­˜å†²çª | Re-run cmake with different source | åŒä¸€ä¸ª build ç›®å½•æ··ç”¨äº†ä¸¤ä¸ª source | æ–°å»º build ç›®å½• |

---

## é™„ï¼šå®Œæ•´å¤ç°å‘½ä»¤ï¼ˆä»é›¶å¼€å§‹ï¼‰

```bash
cd /path/to/Qwen3-ASR-GGUF

# â”€â”€ æ­¥éª¤ 1ï¼šå®‰è£…ä¾èµ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pip3 install torch torchaudio --break-system-packages
pip3 install onnxruntime librosa pydub srt typer rich \
             nagisa sentencepiece accelerate --break-system-packages
pip3 install "transformers==4.57.6" --break-system-packages

# â”€â”€ æ­¥éª¤ 2ï¼šä¸‹è½½å¹¶è§£å‹ GGUF æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mkdir -p model /tmp/gguf_dl
gh release download models --repo HaujetZhao/Qwen3-ASR-GGUF \
    --pattern "Qwen3-ASR-0.6B-gguf.zip" \
    --dir /tmp/gguf_dl --clobber
unzip /tmp/gguf_dl/Qwen3-ASR-0.6B-gguf.zip -d /tmp/gguf_extract/
cp /tmp/gguf_extract/*.onnx /tmp/gguf_extract/*.gguf ./model/

# â”€â”€ æ­¥éª¤ 3ï¼šç¼–è¯‘ libllama.dylib â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
git clone --depth 1 https://github.com/ggml-org/llama.cpp.git /tmp/llama_full
cmake -S /tmp/llama_full -B /tmp/lb \
    -DCMAKE_BUILD_TYPE=Release \
    -DBUILD_SHARED_LIBS=ON \
    -DLLAMA_BUILD_TESTS=OFF \
    -DLLAMA_BUILD_EXAMPLES=OFF \
    -DGGML_METAL=ON \
    -DGGML_BLAS=OFF
cmake --build /tmp/lb --config Release \
    -j$(sysctl -n hw.logicalcpu) --target llama ggml

mkdir -p qwen_asr_gguf/inference/bin
cp /tmp/lb/bin/libllama.dylib \
   /tmp/lb/bin/libggml.dylib \
   /tmp/lb/bin/libggml-base.dylib \
   /tmp/lb/bin/libggml-cpu.dylib \
   /tmp/lb/bin/libggml-metal.dylib \
   qwen_asr_gguf/inference/bin/

# â”€â”€ æ­¥éª¤ 4ï¼šè½¬å½• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python3 transcribe.py test_audio.wav \
    --model-dir ./model \
    --prec int4 \
    --no-dml \
    --no-vulkan \
    --no-ts \
    -y
```

---

## åè®°ï¼šref/llama.cpp ç›®å½•çš„ç”¨é€”

`ref/llama.cpp/` ä¸æ˜¯ç”¨æ¥ç¼–è¯‘ dylib çš„â€”â€”å®ƒä¿ç•™çš„æ˜¯**å¤´æ–‡ä»¶å’Œæºç å‚è€ƒ**ï¼Œ
ç›®çš„æ˜¯å½“é¡¹ç›®ä»£ç éœ€è¦å¯¹é½æ–°çš„ llama.cpp API æ—¶ï¼Œ
å¼€å‘è€…å¯ä»¥åœ¨æœ¬åœ°æŸ¥é˜…å¯¹åº”ç‰ˆæœ¬çš„ `include/llama.h`ï¼Œ
è€Œä¸å¿…æ¯æ¬¡å»ç½‘ä¸ŠæŸ¥ï¼Œä¹Ÿæ–¹ä¾¿ diff æ¯”è¾ƒ API å˜åŒ–ã€‚

å®é™…çš„ dylib åœ¨æ­£å¼å‘å¸ƒæ—¶ä¼šæå‰ç¼–è¯‘å¥½ï¼Œéš Release åŒ…ä¸€èµ·åˆ†å‘ã€‚
æœ¬æ–‡ä¸­æˆ‘ä»¬åœ¨ macOS ä¸Šæ‰‹åŠ¨ç¼–è¯‘ï¼Œæ­£æ˜¯æ¨¡æ‹Ÿäº†è¿™ä¸ªæ‰“åŒ…æµç¨‹ã€‚
